# Metastore

The **Metastore** is a metadata store for PuffinDB tables. It extends the data lake's catalog with metadata that cannot be stored in the latter.

## Partition-Level Column Statistics
The Metastore manages partition-level column statistics that are physically stored on the Object Store (*e.g.* [Amazon S3](https://aws.amazon.com/s3/)). These statistics are critical for helping the [distributed query planner](Query%20Planner.md) perform certain optimizations, or for allowing user interfaces to rapidly display quantitatively-rich previews of tables before querying (*e.g.* [STOIC Table Editor](https://github.com/stoic-doc/Community/discussions/534)).

### Requirements
- Support all major table formats ([Iceberg](https://iceberg.apache.org/), [Delta](https://delta.io/), [Hudi](https://hudi.apache.org/))
- Support tables with fairly large numbers of columns (hundreds or even thousands)
- Support user-defined summary statistics
- Optimize partial lookups for subsets of columns
- Optimize partial lookups for top frequencies
- Optimize partial lookups for histogram subsets
- Keep metadata files as small as possible
- Reduce development costs as much as possible

In order to fulfill these requirements, statistics are captured across three files per partition:
- A Parquet file for summary statistics (minimum, maximum, mean, etc.), with one column per summary statistic and one row per column
- A parquet file the the frequencies of discrete column, with columns for the column, value, and frequency, and one row per column·value
- A parquet file for the histograms of numerical columns, with one column per column and one row per bin (1,000 or 10,000)

This approach would offer the following benefits over the proposed [Iceberg Pufin](https://iceberg.apache.org/puffin-spec/) file format:
- No need to develop and maintain a new parser | serializer
- Low-latency lookup of statistics for specific columns
- Much smaller file size
- Totally independent from [Iceberg Table Format](https://iceberg.apache.org/spec/), therefore compatible with [Delta Lake](https://delta.io/) and [Hudi](https://hudi.apache.org/) table formats.

This approach would offer the following benefits over a key-value store like [DynamoDB](https://aws.amazon.com/dynamodb/) or [Redis](https://redis.io/):
- Lower cost
- Higher scalability (when serving large numbers of concurrent requests)
- Higher throughput (when using up to one serverless function per partition)

### `statistics.parquet`
- One column per summary statistic (minimum, maximum, mean, *etc.*)
- One row per column of the related table
- Ordered as columns are ordered in the related table

This table format is optimized for storing both simple and complex summary statistics (*e.g.* Percentiles).

### `frequencies.parquet`
- Columns for column, value, and frequency
- One row per pair of column·value
- Ordered by column (as columns are ordered in the related table) and decreasing frequency

This table format is optimized for columns with large numbers of distinct values.

### `histograms.parquet`
- One column per column of the related table
- One row per bin (1,000 ot 10,000 bins)
- Ordered by increasing bin minimum value

This table format is optimized for storing high-resolution histograms.

## Table-Level Column Statistics
Table-level column statistics are managed in a similar fashion and are generated by reducing partition-level column statistics.

**Note**: as an option, the table-level `frequencies.parquet` file could include the frequency distribution of every value across every partition. This would dramatically accelerate the identification of partitions that contain certain values for a categorical column. Of course, this would also increase the size of this file, but Parquet makes it easy to retrieve only certain columns, therefore it should not affect the lookup performance of total frequencies (frequencies of values across all partitions). This would also make it more expensive to update this file whenever updates are made to a particular partition, but this overhead might be small in relation to the benefits offered by the consolidation of these frequency distributions. If this feature is implemented, frequency distributions should be stored in two complementary ways: vectors for dense distributions, and hashes for sparse distributions.

## Computation of Frequencies
The computation of table-level frequencies for a column can be very expensive, especially for columns that have a very large number of distinct values. Therefore, this computation is optimized for certain common scenarios. Among them, scenario `#1` where all or almost all values are distinct (*e.g.* primary key), and scenario `#2` where a column has less than 4,294,967,295 (2^32 -1) values. All other scenarios fall into the `#3` bucket, and are implemented with a less efficient algorithm.

To do so, partition-level frequencies are first computed by serverless functions. Their respective numbers of distinct values are then consolidated on the [Monostore](Monostore.md). If all serverless functions reported that all values are distinct for any given partition, the probability that all values are distinct for the entire column is high, and the number of duplicate values across the entire column is low in relation to the total number of values. Be definition, this is scenario `#1`.

In that case, all values are streamed from the serverless functions to the Monostore, and passed through a [Bloom filter](https://en.wikipedia.org/wiki/Bloom_filter). Once all values have been filtered in that fashion, if no duplicates have been detected, we know for certain that all values are distinct across the entire column, because false negatives are not possible with a Bloom filter, and we leave the frequency distribution empty, implicitly indicating that all values are distinct. Otherwise, all possible duplicates (positives, be they true or false) are consolidated on the [Redis](https://redis.io/) instance running on the Monostore, then broadcasted back to all serverless functions. From there, the serverless functions check for actual duplicates, and send back all confirmed duplicates to the Monostore. The resulting frequency distribution only stores duplicate values with their respective counts, while all other values are considered unique implicitly. This multi-step approach improves performance and reduces the size of the resulting frequency distribution by several orders of magnitude, without losing any useful information.

**Note**: this algorithm can be further improved by having serverless functions report that most (not all) values are distinct within their respective partitions. In that case, we know that we have duplicates, but we also know that such duplicates are likely to be few in relation to the total number of values in the column. In that case, we can still implement the algorithm described above to accurately and efficiently identify unique values and detect possible duplicate values.

Otherwise, the Monostore computes the sum of counts of partition-level distinct values. By construction, this sum is greater than or equal to the count of distinct values for the entire column. If this sum is lower than or equal to 4,294,967,295 (2^32 -1), we are in scenario #2. From there, all serverless functions stream their respective partition-level frequency distributions to the Monostore, which consolidates them into a single [Redis hash](https://redis.io/docs/data-types/hashes/). This is possible, because a Redis hash can contain up to 4,294,967,295 field-value pairs. In this hash, the field is used for the column's value, and the value is used for its count. This approach gives us an exhaustive frequency distribution, which includes all values and their counts.

Finally, if the sum of counts of partition-level distinct values is strictly greater than 4,294,967,295, a similar process is implemented in batches: the number of `b` batches is defined by dividing the total number of values by 4,294,967,295. From there, each serverless function streams to the Monostore `b` subsets of its partition-level frequency distribution, ordered by column value. Each batch is then consolidated by the Monostore within a dedicated Redis hash. After counts for values overlaping multiple consecutive batches have been consolidated and hashes have been shuffled to support ordering by decreasing count, we again have an exhaustive frequency distribution, which includes all values and their counts.

**Note**: an implementation of the Bloom filter can be found in [DuckDB](https://duckdb.org/) and was used to implement support for [range joins](https://duckdb.org/2022/05/27/iejoin.html).

## Computations of Ranks
While [quantiles](https://en.wikipedia.org/wiki/Quantile) can be accurately approximated with the most recent versions of the [t-digest](https://github.com/tdunning/t-digest) algorithm, exact quantiles are required for certain applications. In such a case, ranks must be computed for column values through distributed sorting.

## Performance
In most instances, the lookup of statistics for a given partition should take less than 100 ms, and this lookup can be parallelized across 10,000 serverless functions or more. If partitions are 50 MB in size compressed (500 MB uncompressed), 10,000 serverless functions could lookup column statistics for 500 GB of compressed data (5 TB uncompressed) in 100 ms. Moving to partitions that are 1 GB in size compressed (10 GB uncompressed) would let 10,000 serverless functions lookup the same column statistics for 10 TB of data compressed (100 TB uncompressed) within the same 100 ms. This suggests that larger partitions would be preferable (for the Metastore at least).

## Column Statistics Query API
The Metastore provides an API for querying the statistics of table columns. This API is used by the [distributed query planner](Query%20Planner.md) to create a plan for a particular query defined with filtering predicates, and for which the pre-computed column statistics are not sufficient. This API is itself implemented by the [distributed query engine](Query%20Engine.md), in a naturally recursive manner.
